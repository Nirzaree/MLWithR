---
title: "Classification"
author: "Nirzaree"
date: "09/07/2020"
output: html_document
---

recap
load data
plot
split into train test
apply different models
linear classifier
LDA
decision trees (rpart)
knn
svmradial
random forest
plot results 
predict on best model 

functions used: 
1. plotting: featurePlot
2. split train test: createDataPartition
3. fitting models: train 
4. viewing 
3. predicting: predict

config in train: 
train(trainingdata, traininglabels,method, metric, control)
method = "vglmAdjCat":linear classifier, "rpart" : decision trees,"knn","lda","svmRadial","rf"
metric = "accuracy"
control = trainControl(method = "cv", number = 10)

Q: 
Classification models : 
Why are they useful here? 

What do we check to make sure its not misleading? 
How do we choose the validation metric here? 

## Datasets {.tabset}

### Iris

```{r setup,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
library(data.table)
library(caret)
```

```{r loadDataSet,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
#iris is already loaded
data(iris)
```

```{r basicSummary,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
summary(iris)
```

```{r basicPlots,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
featurePlot(iris[,1:4],iris[,5],"pairs")

featurePlot(iris[,1:4],iris[,5],"box")

# featurePlot(iris[,1:4],iris[,5],"ellipse")

featurePlot(iris[,1:4],iris[,5],"density")
```
**Observation**: Petal Width & Length are good features. Sepal length okayish and sepal width very good. 

```{r Classifiermodel_splitData,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}

#split data into test train 
trainingIndex <- createDataPartition(iris[,5],p = 0.8,list = FALSE)

dtTrain <- iris[trainingIndex,]
dtTest <- iris[-trainingIndex,]

```

```{r Classifiermodel_config,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
control <- trainControl(method = "cv",number = 10)
metric <- "accuracy"
```

Models to be tried out:
1. Linear Classifier
2. LDA
3. CART
4 kNN
5. SVM
6. RF

```{r Classifiermodel_tryModels,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
#1. Linear Model
set.seed(3)
fit.linClassifier <- train(dtTrain[,1:4],
                           dtTrain[,5],
                           method = "vglmAdjCat",
                           metric = metric,
                           trControl = control)

set.seed(3)
fit.lda <- train(dtTrain[,1:4],
                 dtTrain[,5],
                 method = "lda",
                 metric = metric,
                 trControl = control)

set.seed(3)
fit.cart <- train(dtTrain[,1:4],
                 dtTrain[,5],
                 method = "rpart",
                 metric = metric,
                 trControl = control)

set.seed(3)
fit.knn <- train(dtTrain[,1:4],
                 dtTrain[,5],
                 method = "knn",
                 metric = metric,
                 trControl = control)

set.seed(3)
fit.svm <- train(dtTrain[,1:4],
                 dtTrain[,5],
                 method = "svmRadial",
                 metric = metric,
                 trControl = control)

set.seed(3)
fit.rf <- train(dtTrain[,1:4],
                 dtTrain[,5],
                 method = "rf",
                 metric = metric,
                 trControl = control)
```

```{r CheckModelAccuracy,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
ClassificationResults <- resamples(list(LinClassifier = fit.linClassifier,
                                        LDA = fit.lda,
                                        CART = fit.cart,
                                        knn = fit.knn,
                                        SVM = fit.svm,
                                        RF = fit.rf))

summary(ClassificationResults)
dotplot(ClassificationResults)
```

```{r InvestigateTheWinningModel,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
print(fit.lda)

```

```{r predictOnValidationSet,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
dtTest <- data.table(dtTest)
dtTest[,predictedClass := predict(fit.lda,newdata =  dtTest)]

confusionMatrix(dtTest[,predictedClass],dtTest[,Species])
```

**Summary:**

### knn-BreastCancenWisconsin
tutorial url: ml-tutorials.kyrcha.#info/knn.html
```{r setupknnbreastcancer,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
library(class)

```


```{r loadDatabreastcancer,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
fileURL <- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"

dtData <- data.table(read.csv(fileURL,header = FALSE))
str(dtData)
```

```{r PreProcessbreastcancer,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
#first column is id column which is not required
dtData <- dtData[,-1]

#put names in the columns
names(dtData) <- c("ClumpThickness",
"UniformityCellSize",
"UniformityCellShape",
"MarginalAdhesion",
"SingleEpithelialCellSize",
"BareNuclei",
"BlandChromatin",
"NormalNucleoli",
"Mitoses",
"Class")

#V7 is char. because ? whenever missing data. 
dtData[,BareNuclei := ifelse(BareNuclei == "?",NA,BareNuclei)]
dtData[,BareNuclei := as.integer(BareNuclei)]
str(dtData)

#make output categorical
dtData[,Class := factor(Class,levels = c(2,4),labels = c("benign","malignant"))]

print(summary(dtData))

#check how balanced is the dataset
print(summary(dtData$Class))

```

Note: Since range of all feature columns are same,we can probably get away with standardizing input data. 

```{r TrainTestSplitbreastcancer,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
trainingIndices <- sample(seq(1:nrow(dtData)),0.7*nrow(dtData))
dtTrain <- dtData[trainingIndices,]
dtTrain <- dtTrain[complete.cases(dtTrain),]
dtTrainX <- dtTrain[,1:9]
dtTrainY <- dtTrain$Class #dtTrain[,10]  this doesnt work because labels have to be a vector.
dtValidation <- dtData[-trainingIndices,]
dtValidation <- dtValidation[complete.cases(dtValidation),]
dtValidationX <- dtValidation[,1:9]
dtValidationY <- dtValidation$Class  #dtValidation[,10] same as above
```

```{r Modelbreastcancer,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
Predictions <- knn(train = dtTrainX,test = dtValidationX,cl = dtTrainY,k = 1)
```

```{r Validatebreastcancer,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
dtConfusionMatrix <- table(Predictions,dtValidationY)

#how to choose the positive class ? 
#class imbalance? 
#Accuracy
Accuracy = sum(Predictions == dtValidationY)/length(dtValidationY)

#Precision
Precision = dtConfusionMatrix[2,2]/(dtConfusionMatrix[2,2] + dtConfusionMatrix[2,1])

#Recall
Recall = dtConfusionMatrix[2,2]/(dtConfusionMatrix[2,2] + dtConfusionMatrix[1,2])
#F1 score

```


```{r CrossValidation,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
PredictionsknnCV <- knn.cv(train = dtTrainX,cl = dtTrainY,k = 1)

table(PredictionsknnCV,dtTrainY)

sum(PredictionsknnCV == dtTrainY)/length(dtTrainY)
```

https://courses.cognitiveclass.ai/courses/course-v1:BigDataUniversity+ML0151EN+2017/courseware/76d637cbe8024e509dc445df847e6c3a/d2529e01786a412fb009daef4b002a48/1?activate_block_id=block-v1%3ABigDataUniversity%2BML0151EN%2B2017%2Btype%40vertical%2Bblock%40356f36b091764fa9a43ccb737e586563

New function alert: expand.grid : 

```{r ChoosekValue,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
# modelControl <- trainControl(method = "cv",
                             # number = 10)

modelControl <- trainControl(method = "repeatedcv",
                             number = 5,
                             repeats = 2)
knngrid <- expand.grid(k = c(1,3,5,7,9))

knn_k_search <- train(Class~.,
                      data = dtTrain,
                      method = "knn",
                      trControl = modelControl,
                      preProcess = c("center","scale"),#doing this is changing the results between this and default knn.
                      tuneGrid = knngrid)

knn_k_search
```

```{r defaultknncaretfit,}

control <- trainControl(method = "cv",number = 10)
metric <- "accuracy"
defaultknn <- train(Class~.,
                 data = dtTrain,
                 method = "knn",
                 metric = metric,
                 trControl = control)

summary(defaultknn)
```

Not a lot of changes with k values. What does that tell abotu the dataset? 


Cross validation : caret easy settings
optimal k value : caret does it
Validation metrics : depends on the dataset I guess

Summary: 
knn :
caret or class
in class u specify k value
check if overfitting by doing confusion matrix

choosing k value 
all in train function in caret
metric
method
training control 
preprocess

Preprocess: center, scale, pca. 

Validate: using accuracy,precision,recall etc
Handle imbalance in class: ? Dont know really But can still chck if model is performing 

how to tell the model abouot the positive class. 


