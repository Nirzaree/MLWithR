---
title: "Classification"
author: "Nirzaree"
date: "09/07/2020"
output:
  html_document:
    fig_caption: true
    toc: true
    toc_float: true
    toc_collapsed: true
toc_depth: 3
---

## Goal: Understand classification concepts with examples

## Theory

* What is classification? 
  A technique applied to estimate categorical output variable
* What types of classification techniques exist? 
  + Binary
  + Multiclass
* (A few) Classification algorithms
  + Decision Trees
  + Linear Discriminant Analysis (LDA)
  + Random forest
  + Support Vector Machine (SVM)
  + k-nearest neighbors (kNN)
  + Naive Bayes
* Assumptions of classification
  None that I can think of 
* Preprocessing for classification
* Feature Engineering for classification
* Tuning a classification model
* Metrics for evaluating a classification model
  + Binary Classifier: 
    + Confusion matrix
    + AUC-ROC Curve
    + Precision, Recall
    + Sensitivity, Specificity
  + Multiclass Classifier:
  
We now look at a couple of datasets

## Iris

```
Dataset description: Edgar Anderson's Iris Data

This famous (Fisher's or Anderson's) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.

iris is a data frame with 150 cases (rows) and 5 variables (columns) named Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, and Species.

Task: Classification of iris species from the sepal & petal length and width features.
```

```{r setup,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
library(data.table)
library(caret)
```

### 1. Load Data
```{r loaddata1,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
#iris is already loaded
data(iris)
```

```{r basicSummary,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
summary(iris)
```

### 2. Feature Exploration
A couple of different ways of feature plots: 
i) pair plot
ii) box plot
iii) density plot
 
```{r basicPlots,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
featurePlot(iris[,1:4],iris[,5],"pairs")

featurePlot(iris[,1:4],iris[,5],"box")

# featurePlot(iris[,1:4],iris[,5],"ellipse")

featurePlot(iris[,1:4],iris[,5],"density")
```
**Observation**: Petal Width & Length are good features. Sepal length okayish and sepal width not good. 

### 3. Build model: a) Train-test split
```{r Classifiermodel_splitData,include=FALSE,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
#split data into test train 
trainingIndex <- createDataPartition(iris[,5],p = 0.8,list = FALSE)

dtTrain <- iris[trainingIndex,]
dtTest <- iris[-trainingIndex,]
```

### 3. Build model: b) Training config:
                        method = 10 fold cross validation (todo: how many repeats?)
                        metric = accuracy
```{r Classifiermodel_config,include=FALSE,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
control <- trainControl(method = "cv",
                        number = 10)
metric <- "accuracy"
```

### 3. Build model: c) Build multiple models
1. Linear Classifier
2. LDA
3. CART (Decision tree)
4. kNN
5. SVM
6. RF
7. Naive Bayes

We try these classification models using caret package's train function
```{r Classifiermodel_tryModels,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
AllModels <- fMultipleClassificationModels(TrainingData = dtTrain[,1:4],
                              TrainingLabels = dtTrain[,5],
                              TrainingControl = control,
                              EvalMetric = metric,
                              TrainingModels =  c("vglmAdjCat","lda","rpart","knn","svmRadial","rf","nb")
                              )
```

### 4. Identify the best model
```{r CheckModelAccuracy,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
ClassificationResults <- resamples(list(LinClassifier = AllModels[[1]],
                                        LDA = AllModels[[2]],
                                        CART = AllModels[[3]],
                                        knn = AllModels[[4]],
                                        SVM = AllModels[[5]],
                                        RF = AllModels[[6]],
                                        NB = AllModels[[7]]))
summary(ClassificationResults)
dotplot(ClassificationResults)
```
**Observation:** Linear classifier has the highest accuracy overall.

### 5. Investigate the best model
```{r InvestigateTheWinningModel,include=FALSE,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
fit.linClassifier = AllModels[[1]]
print(fit.linClassifier)
print(fit.linClassifier$finalModel)
varImp(fit.linClassifier)
```
**Observation:** Variable importance in the model is matching our intuition from feature plots. 
### 6. Predict on test data
```{r predictOnValidationSet,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
dtTest <- data.table(dtTest)
dtTest[,predictedClass := predict(fit.linClassifier,newdata =  dtTest)]

confusionMatrix(dtTest[,predictedClass],dtTest[,Species])
```
**Observation:**
* 1 sample of virginica is being classified as versicolor by the model which results in reduced sensitivity towards the virginica class and slightly reduced specificity of the versicolor class.
* Balanced Accuracy of each of the classes are: Setosa (1), Versicolor (0.975), Virginica (0.95)
* Overall accuracy of the model is : 29/30 = 0.9667

**Summary:**
* This is a beautiful dataset with respect to:
  1. Having completely balanced classes
  2. All features having the same range and units
which eliminates a lot of preprocessing. 
* We tried a few classification models on the dataset after splitting the dataset into an 80:20 train:test split with 10 fold cross validation.
* Linear Classifier had the highest accuracy among all the other models, hence we used it to make predictions on the remaining 20% data.
* Prediction accuracy on the remaining 20% data is ~97% (only 1 sample misclassified)
* Variable importance in each of the models is reflecting what appears from feature plots of the features. We looked at the variable importance on the linear classifier model. Petal Width & Length being important features followed by sepal length followed by sepal width.

We now look at another dataset: Titanic 

## Titanic

```
Dataset description: Survival of passengers on the Titanic

This data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner ‘Titanic’, summarized according to economic status (class), sex, age and survival.

Format
A 4-dimensional array resulting from cross-tabulating 2201 observations on 4 variables. The variables and their levels are as follows:

No	Name	Levels
1	Class	1st, 2nd, 3rd, Crew
2	Sex	Male, Female
3	Age	Child, Adult
4	Survived	No, Yes

Task: Binary classification
```

```{r loadData2,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
library(rattle)
library(rpart)
library(rpart.plot)

# library(qwraps2)
# options(qwraps2_markup = "markdown")

mosaicplot(Titanic)
```
**High Level summary of the dataset**
1. Sex: Male, Female
2. Class: 1, 2, 3, Crew
3. Age: Adult, Child

Understanding of survival based on the above factors from the mosaic plot: 
1. Higher Survival Rates:
  + 1st class + Female + (Any age) (mostly all survived)
  + 2nd class + Female + adult (majority survived)
  + 2nd class + Female + child (all survived)
2. Low survival rates:  
  + 3rd class + Male + (Any age)
  + 2nd class + Male + Adult

```{r preprocessing,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 8, fig.height = 4}
temp <- data.table(Titanic)
dtTitanic <- temp[as.numeric(rep(row.names(temp),temp$N)),] #0 frequency rows are gone now. 
dtTitanic[,N := NULL]

#convert categorical variables into factors 
dtTitanic[,Class := as.factor(Class)] 
dtTitanic[,Sex := as.factor(Sex)]
dtTitanic[,Survived := as.factor(Survived)]
dtTitanic[,Age := as.factor(Age)]

summary(dtTitanic)
```

### 3. Build model: a) Train-test split
```{r Classifiermodel_splitData2,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}

#split data into test train 
trainingIndex <- sample(seq(1:nrow(dtTitanic)),0.8*nrow(dtTitanic))

dtTitanicTrain <- dtTitanic[trainingIndex,]
dtTitanicTest <- dtTitanic[-trainingIndex,]
```

### 3. Build model: b) Training config:
                        method = 10 fold cross validation (todo: how many repeats?)
                        metric = accuracy
```{r Classifiermodel_config2,include=FALSE,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
control <- trainControl(method = "cv",
                        number = 10)
metric <- "accuracy"
```

### 3. Build model:
Distance based models are out.
We try a decision tree. We tune it with 2 different cp values 0.005,0.01 (default) & look at the tree logic.
```{r TitanicModel1,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
# lFormula <- paste0(names(dtTitanic)[4],"~",paste(names(dtTitanic)[1:3],collapse = "+"))
Titanic.cart <- train(Survived~Class+Sex+Age,
                      data = dtTitanicTrain,
                      method = "rpart",
                      trControl = control,
                      tuneGrid =  expand.grid(cp = c(0.005,0.01)),
                      metric = metric)

fancyRpartPlot(Titanic.cart$finalModel)
(Titanic.cart)
```
**Observation:** 
Tree logic looks good. 
This is  the max information that can be extracted from the categorical dataset available.
Accuracy of the tree on training data with cp = 0.005: ~78% 

### 4. Predict on test data
```{r Titanicpredict1,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
dtTitanicTest[,"predictedClass"] <- predict(Titanic.tunedcart,
                                       dtTitanicTest,
                                       type = "class")

confusionMatrix(dtTitanicTest[,predictedClass],dtTitanicTest[,Survived])
```
**Observations:**
Accuracy of the model is decent ~82% however it is quite poorly classifying the survived class. The accuracy number is still high because of the class imbalance (~2/3 not survived class). Balanced accuracy is 72%. 

We now look at a more detailed Titanic dataset. 

## ToDo / ToAnswer
**Iris**
1. understand kappa
2. CI
3. What other metric apart from accuracy can be used here
**Titanic**
4. Which classification models are distance based and wont work with data with all categorical variables? 
5. rpart vs caret - rpart
6. resamples ran on titanic without setting the seed for each models. Why? [Titanic]
7. How to broadly know how much accuracy can the dataset achieve on models? Like if features are just bad, then no point trying tuning models. 
8. ctree vs rpart
4. find max accuracy on base titanic dataset (only 3 variables: sex,class,age)

Validate: using accuracy, precision, recall etc
Handle imbalance in class: ? Dont know really But can still chck if model is performing 
how to tell the model about the positive class. 

## notes to self
1. Beautiful function that works on tables: mosaicplot : Gave all info of Titanic dataset in 1 plot.
2. Not best summary table but still: qwrap2 library's summary_table (qwrap2_markup = "markdown" needs to be configured)
3. Important function: resamples : Collation and Visualization of Resampling Results
   resamples checks that the resampling results match; that is, the indices in the object trainObject$control$index are the same. Also, the argument trainControl returnResamp should have a value of "final" for each model. 
The summary function computes summary statistics across each model/metric combination. [https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/resamples](https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/resamples)        


