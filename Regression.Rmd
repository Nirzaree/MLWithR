---
title: "Regression"
author: "Nirzaree"
date: "31/08/2020"
output: 
  html_document:
    fig_caption: true
    toc: true
    toc_float: true
    toc_collapsed: true
toc_depth: 3
---
have the intuitive nondomain approach up:
next domain + intuition approach
domain + intuition + best practice 

Goal: Understand regression concepts with examples

## Theory
* What is regression?   
  A technique applied to estimate a continuous output variable as a function of one or more input variables.

* What types of regression techniques exist? 

  1. Linear: $y = a_0 + a_1*x_1 + a_2*x_2 + a_3*x_3 + \epsilon$    
      + Univariate  
      + Multivariate  
  2. Polynomial: $y = a_0 + a_1*x_1 + a_2*(x_1)^2 + a_3*(x_2) + a_4*(x_2)^2 + a_5*x_1*x_2 + \epsilon$
  3. Logistic: $log(p_i/(1-p_i)) = a_0 + a_1*x_1 + a_2*x_2 + a_3*x_3 + \epsilon$

<!-- * Assumptions of linear regression -->

<!-- * Metrics for evaluaring a linear regression model -->

## Intuitive Approach
Intuitively we would do the following (with no theory knowhow on feature engineering,validation of model etc) 

1. Load the dataset & check if there are features that correlate significantly and linearly (well we can transform the input data and it has its nuances but for now we just try to see what best fits the data as it is) the output/response variable
3. Build a model with all variables 
4. Check for important variables
5. Reduce the model to include only the significant variables
6. Check how good the model is: 
7. If it is good enough, use it to make predictions

## MTCars Dataset

```
Dataset description: mtcars
The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973â€“74 models).

Format: 
A data frame with 32 observations on 11 (numeric) variables.

[, 1]	mpg	Miles/(US) gallon
[, 2]	cyl	Number of cylinders
[, 3]	disp	Displacement (cu.in.)
[, 4]	hp	Gross horsepower
[, 5]	drat	Rear axle ratio
[, 6]	wt	Weight (1000 lbs)
[, 7]	qsec	1/4 mile time
[, 8]	vs	Engine (0 = V-shaped, 1 = straight)
[, 9]	am	Transmission (0 = automatic, 1 = manual)
[,10]	gear	Number of forward gears
[,11]	carb	Number of carburetors

Task: Regression analysis of fuel efficiency

Reference: https://rstudio-pubs-static.s3.amazonaws.com/157017_d791d59b482441d386ce8b31b3337ecf.html
```
### Intuitive approach
```{r setup1,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
library(data.table)
library(corrplot)
library(ggplot2)
library(ggpmisc)
library(grid)
library(gridExtra)
```

### 1. Load Data
```{r loaddata1,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
dtMTCars <- data.table(mtcars)
head(dtMTCars)

summary(dtMTCars)
```

**High level summary**   

* Predictor variables: 10  
  + Numeric variables: 8  
  + Categorical variables: 2 (Engine (0 = V-shaped, 1 = straight) & Transmission (0 = automatic, 1 = manual))     
* Response variable: mpg	Miles/(US) gallon  

### 2. Feature Engineering
Let's plot the output variable against each of the input variables.
We also plot best fit polynomial for each variable. 

```{r featureanalysis,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 8, fig.height = 4}
# corrplot(cor(dtMTCars))

#some more plots
genericformula = y~x

cylplot <- ggplot(dtMTCars,aes(x = cyl,y = mpg)) + geom_point() + geom_smooth(formula = genericformula,method = "lm") + ggtitle("MPG Vs number of cylinders") + stat_poly_eq(formula = genericformula,aes(label = paste(..eq.label.., ..rr.label.., sep =  "~~~")), parse = TRUE) 

dispplot <- ggplot(dtMTCars,aes(x = disp,y = mpg)) + geom_point() + geom_smooth(formula = genericformula,method = "lm") + ggtitle("MPG Vs displacement") + stat_poly_eq(formula = genericformula,aes(label = paste(..eq.label.., ..rr.label.., sep =  "~~~")), parse = TRUE) 

hpplot <- ggplot(dtMTCars,aes(x = hp,y = mpg)) + geom_point() + geom_smooth(formula = genericformula,method = "lm") + ggtitle("MPG Vs horsepower") + stat_poly_eq(formula = genericformula,aes(label = paste(..eq.label.., ..rr.label.., sep =  "~~~")), parse = TRUE) 

dratplot <- ggplot(dtMTCars,aes(x = drat,y = mpg)) + geom_point() + geom_smooth(formula = genericformula,method = "lm") + ggtitle("MPG Vs rear axle ratio") + stat_poly_eq(formula = genericformula,aes(label = paste(..eq.label.., ..rr.label.., sep =  "~~~")), parse = TRUE) #not great relation

grid.arrange(cylplot,dispplot,hpplot,dratplot,nrow = 2)

wtplot <- ggplot(dtMTCars,aes(x = wt,y = mpg)) + geom_point() + geom_smooth(formula = genericformula,method = "lm") + ggtitle("MPG Vs weight") + stat_poly_eq(formula = genericformula,aes(label = paste(..eq.label.., ..rr.label.., sep =  "~~~")), parse = TRUE) 

qsecplot <- ggplot(dtMTCars,aes(x = qsec,y = mpg)) + geom_point() + geom_smooth(formula = genericformula,method = "lm") + ggtitle("MPG Vs qsec") + stat_poly_eq(formula = genericformula,aes(label = paste(..eq.label.., ..rr.label.., sep =  "~~~")), parse = TRUE) #positive relation but quite a lot of variation.

vsplot <- ggplot(dtMTCars,aes(x = vs,y = mpg)) + geom_point() + geom_smooth(formula = genericformula,method = "lm") + ggtitle("MPG Vs engine type") + stat_poly_eq(formula = genericformula,aes(label = paste(..eq.label.., ..rr.label.., sep =  "~~~")), parse = TRUE)

amplot <- ggplot(dtMTCars,aes(x = am,y = mpg)) + geom_point() + geom_smooth(formula = genericformula,method = "lm") + ggtitle("MPG Vs transmission type") + stat_poly_eq(formula = genericformula,aes(label = paste(..eq.label.., ..rr.label.., sep =  "~~~")), parse = TRUE) 

grid.arrange(wtplot,qsecplot,vsplot,amplot,nrow = 2)

gearplot <- ggplot(dtMTCars,aes(x = gear,y = mpg)) + geom_point() + geom_smooth(formula = genericformula,method = "lm") + ggtitle("MPG Vs number of forward gears") + stat_poly_eq(formula = genericformula,aes(label = paste(..eq.label.., ..rr.label.., sep =  "~~~")), parse = TRUE)

carbplot <- ggplot(dtMTCars,aes(x = qsec,y = mpg)) + geom_point() + geom_smooth(formula = genericformula,method = "lm") + ggtitle("MPG Vs number of carburators") + stat_poly_eq(formula = genericformula,aes(label = paste(..eq.label.., ..rr.label.., sep =  "~~~")), parse = TRUE) #not great relation

grid.arrange(gearplot,carbplot,nrow = 1)

```

<!-- **Observations:** -->
<!-- MPG against -->
<!-- 1. Number of cylinders: As number of cylinders increase, MPG is reducing. However number of cylinders being a discrete variable with only 3 values in the dataset, would not be able to discern mpg on a finer scale. -->
<!-- 2. Weight: Inversely proportional with mpg. Linear relation. Weight being a continuous variable is nicely fitting the mpg line.  -->
<!-- 3. Transmission: Transmission type 1 (manual) overall has higher mpg than class 0 (automatic) however there is a lot of overlapping data meaning that other variables would be required for better mpg prediction along with transmission type. -->
<!-- 4.  -->

### 3. Build model: Simple linear regression
We now build a model using all variables and we check how it looks. 

understand the summary stats. 
choose features from here.

```{r AllVarModel,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
MultLinearModel <- lm(mpg~.,data = dtMTCars)
summary(MultLinearModel)

par(mfrow=c(2,2))
plot(MultLinearModel)
```

**Observations:**
Score: R2 adjusted is 80.66% which is decent.
Assumptions are fairly satisfied. 
However, we might not need all these variables in the model. 

### 3. Build model: Simple linear regression
We now build a model using a few variables that seem to have a good correlation with the output variable. (how are we checking that there is not a lot of collinearity between the variables? : from adjusted R2 of the)
```{r SomeVarModel,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
MultLinearModel <- lm(mpg~wt+cyl+disp,data = dtMTCars)
summary(MultLinearModel)
```

**Observatiions:**
Score: R2 adjusted: 81.47% slightly better than the model with all variables. Why so? Because that model was not able to remove variables. Now we let it do that. We try stepwise linear regression. 

### 3. Build model: Stepwise linear regression
```{r StepwiseModel,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
LinModelStep <- step(lm(mpg~.,dtMTCars),direction = "both")
summary(LinModelStep)
```

### 4. Validate best available model
Stepwise linear model looks the best among what we explored so far, so we check if the linear regression assumptions are validated by the model. 

```{r ValidateModel,echo = FALSE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10}
par(mfrow=c(2,2))
plot(LinModelStep)
```

**Observations:**
We now figure if linear regression assumptions are validated by the model for the data.
Linear Regression assumptions:
1. Linearity: 
2. Constant error variance (homoscedasticity)
3. Independent error terms (this is difficult to identify through simple plots, so we leave it aside for now)
4. Normal errors

1. Residuals Vs fitted: 
  * No pattern in residuals vs fitted values. In a linear regression, sum of residuals would be 0. There would be no pattern in the error terms and the points should just be a random cloud of points => Constant variance assumption met
  * Red line is almost horizontal (not great but not too bad) => linearity assumption met
2. Q-Q plot: Follow the line for the most part  => normally distributed error terms assumption met

Other plots that help in identifying non-linearity, non constant variance and other problematic observations, but we chill on that for now
3. Variance in residuals: Fairly flat line hence variance is reasonably constant across predictor range (homoscedasticity)
4. Cooke's distance : for the 3 higher points is < 0.5.
Threshold Generally: 4/N or 4/(n - k -1) where k = number of explanatory variables & N = number of observations.
Need to check about how much cooke's distance is bad, with respect to the given problem.


### 5. Summary
We took dataset with multiple explanatory variables and one response variable of interest.
We applied a linear model using all variables.
 It fairly satisfied the assumptions however seemed to have too many variables in it, which might not be required.
 We then tried the model with a few variables which seemed to have linear relation with the output variable, and the model had slightly better R2 than the model with all the variables, suggesting further that all variables are not required. 
 We finally employed stepwise linear regression model which has the capability of removing unnecessary variables. It had sligtly better R2 than both the earlier models. 
 We then checked if the assumptions of linearity, constant error variance, normal distribution of errors are satisfied by the final (step wise linear) model and they were fairly satisfied.
 What is not been covered in this notebook is how to handle collinearity between features, how to find the best linear model, any pre-processing requirements for linear model. 

<!-- Finally, stepwise linear regression with CV:  -->
<!-- <!-- https://topepo.github.io/caret/train-models-by-tag.html for caret model names-->  -->

<!-- ### 5. Build model: Stepwise linear regression with cross validation -->
<!-- ```{r WithCV,echo = TRUE, message = FALSE, warning = FALSE,fig.width = 16, fig.height = 10} -->
<!-- library(caret) -->

<!-- trainingControl <- trainControl(method = "cv",number = 10) -->

<!-- LinModelWithCV <- train(mpg~., -->
<!--                         data = dtMTCars, -->
<!--                         method = "lmStepAIC", -->
<!--                         trControl = trainingControl -->
<!--                       ) -->

<!-- summary(LinModelWithCV) -->

<!-- LinModelWithCV$results -->

<!-- summary(LinModelWithCV$finalModel) -->
<!-- #with DAAG library's function:  -->
<!-- # cv.lm(data = dtMTCars,mpg~.) -->
<!-- ``` -->

## Summary

todo: understand results of cv
understand params in cross validation folds, iterations
formatting
high level summary: dataset params :categorical variables numeric variables

## notes to self
1. new package for plotting equation in feature plots
  + library: ggpmisc 
  + function: stat_poly_eq
